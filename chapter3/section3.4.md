## 小系统到大系统

正如摩尔定律预测的那样，计算能力一直在以惊人的速度增长，并且没有出现放缓的迹象。 任何高端服务器只包含一个 CPU 的情况相对较少。 这是通过许多不同的方式实现的。

### 对称多处理(Symmetric Multi-Processing)

对称多处理（通常缩写为 SMP）是目前在单个系统中包含多个 CPU 的最常见配置。

`symmetric`术语指的是系统中的所有 CPU 都是相同的（例如架构，时钟速度）。 在 SMP 系统中，有多个处理器共享其他所有其他系统资源（内存，磁盘等）。

### 缓存一致性(Cache Coherency)

在大多数情况下，系统中的 CPU 独立工作;每个都有自己的一组寄存器，程序计数器等。尽管单独运行，但有一个组件需要严格的同步。

这是 CPU 缓存;记住缓存是一个快速访问内存的小区域，它反映了存储在主系统内存中的值。如果一个 CPU 修改主存储器中的数据而另一个 CPU 在其高速缓存中具有该存储器的旧副本，则系统显然不会处于一致状态。请注意，问题仅在处理器写入内存时发生，因为如果仅读取值，则数据将保持一致。

为了协调在所有处理器上保持高速缓存一致性，SMP 系统使用侦听(snooping)。侦听(snooping)是处理器侦听总线的地方，所有处理器都连接到该总线以进行缓存事件，并相应地更新其缓存。

一个执行此操作的协议是 MOESI 协议;代表修改（Modified），所有者（Owner），独家（Exclusive），共享（Shared），无效（Invalid）。这些中的每一个都是高速缓存行可以位于系统中的处理器上的状态。还有其他协议可以做多少，但它们都有相似的概念。下面我们检查 MOESI，以便您了解该过程需要什么。

当处理器需要从主存储器读取高速缓存行时，它首先必须听(窥探,snooping)系统中的所有其他处理器以查看它们当前是否知道有关该存储器区域的任何信息（例如，将其高速缓存）。如果在任何其他进程中不存在，则处理器可以将内存加载到缓存中并将其标记为独占。当它写入缓存时，它会更改要修改的状态。这里缓存的具体细节发挥作用;一些缓存会立即将修改后的缓存写回系统内存（称为直写缓存，因为写操作会进入主内存）。其他人不会，并且只有当缓存被驱逐时才会将修改后的值保留在缓存中，例如缓存变满。

另一种情况是处理器听(snooping)并发现该值在另一个处理器缓存中。如果此值已标记为已修改，则会将数据复制到其自己的缓存中并将其标记为已共享。它将为另一个处理器（我们从中获取数据）发送一条消息，将其缓存行标记为所有者。现在假设系统中的第三个处理器也想使用该内存。它将窥探并找到共享和所有者副本;因此，它将从所有者价值中获取其价值。虽然所有其他处理器仅读取该值，但缓存行仍保持在系统中共享。但是，当一个处理器需要更新该值时，它会通过系统发送无效消息。具有该缓存行的任何处理器必须将其标记为无效，因为它不再反映“真实”值。当处理器发送 invalidate 消息时，它会在其缓存中将缓存行标记为已修改，而其他所有缓存行都将标记为无效（请注意，如果缓存行是独占的，则处理器知道没有其他处理器依赖它，因此可以避免发送消息无效）。

从这一点开始，整个过程就开始了。因此，无论哪个处理器具有修改后的值，都有责任在从缓存中逐出时将真值写回 RAM。通过思考协议，您可以看到这确保了处理器之间缓存线的一致性。

随着处理器数量的增加，该系统存在一些问题。只有少数处理器，检查另一个处理器是否具有高速缓存行（读取监听）或使每个其他处理器中的数据无效（使监听无效）的开销是可管理的;但随着处理器数量的增加，总线流量也会增加。这就是 SMP 系统通常只能扩展到大约 8 个处理器的原因。

将处理器全部放在同一总线上也开始出现物理问题。导线的物理特性仅允许它们彼此以一定距离布置并且仅具有一定长度。对于运行在许多千兆赫兹的处理器，光速开始成为消息在系统中移动需要多长时间的真正考虑因素。

请注意，系统软件通常不参与此过程，尽管程序员应该了解硬件在其下面做了什么，以响应他们设计的程序以最大限度地提高性能。

#### 在 SMP 系统中的缓存独占性（Cache exclusivity in SMP systems）

在名为“[深度缓存](http://www.bottomupcs.com/memory.xhtml#cache_in_depth)”的部分中，我们描述了包含 v 的独占缓存。通常，L1 高速缓存通常是包容性的(inclusive) - 即 L1 高速缓存中的所有数据也驻留在 L2 高速缓存中。在多处理器系统中，包含 L1 高速缓存意味着只有 L2 高速缓存需要窥探存储器流量以保持一致性，因为 L2 中的任何变化都将保证由 L1 反映。这降低了 L1 的复杂性，并使其与窥探过程脱离，从而使其更快。

同样，通常，大多数现代高端（例如，不针对嵌入式）处理器具有用于 L1 高速缓存的直写策略，以及用于较低级高速缓存的回写策略。有几个原因。由于在这类处理器中 L2 高速缓存几乎完全在片上并且通常非常快，因此具有 L1 写入的惩罚不是主要考虑因素。此外，由于 L1 大小很小，将来不太可能读取的写入数据池可能导致有限 L1 资源的污染。另外，如果具有突出的脏数据，则不必关注直写 L1，因此可以将额外的一致性逻辑传递给 L2（正如我们所提到的，已经在`缓存一致性`大部分已经阐述了）。

#### 超线程(Hyperthreading)

现代处理器的大部分时间都花在等待存储器层次结构中较慢的设备上以提供用于处理的数据。

因此，保持处理器流水线充满的策略是至关重要的。 一种策略是包括足够的寄存器和状态逻辑，以便可以同时处理两个指令流。 这使得一个 CPU 可以查找所有意图和目的，例如两个 CPU。

虽然每个 CPU 都有自己的寄存器，但它们仍然必须共享 CPU 的核心逻辑，高速缓存以及输入和输出带宽到内存。 因此，虽然两个指令流可以使处理器的核心逻辑更加繁忙，但是具有两个物理上独立的 CPU 的性能提升不会那么大。 通常情况下，性能提升低于 20％（XXX 检查），但根据工作负载的不同，性能可能会大大提高或提高。

#### Multi Core

随着越来越多的能够在芯片上安装越来越多的晶体管，可以将两个或更多处理器放在同一个物理封装中。最常见的是双核，其中两个处理器核心在同一芯片中。与超线程不同，这些内核是完整的处理器，因此在 SMP 系统中看起来像两个物理上独立的处理器。

虽然通常处理器具有自己的 L1 高速缓存，但它们必须共享连接到主存储器和其他设备的总线。因此，性能不如完整的 SMP 系统那么大，但是比超线程系统要好得多（事实上，每个核心仍然可以实现超线程以进行额外的增强）。

多核处理器还具有一些与性能无关的优点。正如我们所提到的，处理器之间的外部物理总线具有物理限通过将处理器包含在彼此非常接近的同一块硅片上，可以解决这些问题中的一些问题。多核处理器的功率要求远低于两个独立处理器。这意味着需要耗散的热量更少，这在数据中心应用中是一个很大的优势，在这些应用中，计算机被打包在一起并且冷却考虑因素可能相当大。通过将内核置于相同的物理封装中，可以在多种情况下实现多处理，例如笔记本电脑。仅生产一个芯片而不是两个芯片也要便宜得多。

#### Clusters

许多应用程序要求系统远大于 SMP 系统可扩展到的处理器数量。进一步扩展系统的一种方法是群集。

群集只是一些具有相互通信能力的个人计算机。在硬件级别，系统彼此不了解;将各个计算机拼接在一起的任务由软件决定。

诸如 MPI 之类的软件允许程序员编写他们的软件，然后将程序的一部分“移出”到系统中的其他计算机。例如，映像执行数千次执行独立操作的循环（即循环的迭代不会影响任何其他迭代）。通过群集中的四台计算机，该软件可以使每台计算机各执行 250 次循环。

计算机之间的互连各不相同，可能与互联网链接一样慢，也可能与专用的专用总线（Infiniband）一样快。然而，无论互连是什么，它仍然会在存储器层次结构中进一步下降，并且比 RAM 慢得多。因此，当每个 CPU 需要访问可能存储在另一台计算机的 RAM 中的数据时，集群将不能很好地执行;因为每次发生这种情况，软件都需要从另一台计算机请求数据的副本，在处理器完成任何工作之前，通过慢速链接复制到本地 RAM。

但是，许多应用程序不需要在计算机之间进行这种不断的复制。一个大规模的例子是 SETI @ Home，其中分析从无线电天线收集的数据的外来生命迹象。每台计算机都可以分发几分钟的数据进行分析，只需要报告它找到的内容的摘要。 SETI @ Home 实际上是一个非常庞大的专用集群。

另一个应用是渲染图像，尤其是电影中的特殊效果。每台计算机都可以放在电影的一个帧中，其中包含线框模型，纹理和光源，需要将它们组合（渲染）成我们现在用于粒度的惊人特效。由于每个帧都是静态的，因此一旦计算机具有初始输入，则在最终帧准备好被发送回并组合到移动中之前，它不需要任何更多的通信。例如，“魔戒指南”（Lord of the Rings）在运行 Linux 的巨大集群上呈现了它们的特效。

#### 非统一内存访问(Non-Uniform Memory Access)

非统一内存访问，通常缩写为 NUMA，几乎与上面提到的集群系统相反。 在集群系统中，它由链接在一起的各个节点组成，但节点之间的链接是高度专业化的（并且非常昂贵！）。 与硬件不知道节点之间的链接的集群系统相反，在 NUMA 系统中，软件没有（很少，更少）关于系统布局的知识，并且硬件完成将节点链接在一起工作。

术语非统一内存访问来自 RAM 可能不是 CPU 本地的事实，因此可能需要从一定距离的节点访问数据。 这显然需要更长的时间，与单个处理器或 SMP 系统形成对比，在单个处理器或 SMP 系统中，RAM 直接连接并始终需要恒定（统一）的时间来访问。

#### NUMA 机器布局

由于在系统中有如此多的节点相互通信，因此最小化每个节点之间的距离至关重要。 显然，最好是每个节点都有到每个其他节点的直接链接，因为这样可以最大限度地减少任何一个节点查找数据所需的距离。 当节点数量开始增长到数百和数千时，这不是一个实际情况，就像大型超级计算机一样; 如果你还记得你的高中数学问题基本上是一次两个组合（每个节点与另一个节点交谈），并且会增长 n！/ 2 \*（n-2）！

为了对抗这种指数增长，替代布局用于权衡节点之间的距离和所需的互连。 现代 NUMA 架构中常见的一种布局是超立方体。

超立方体具有严格的数学定义（超出此讨论的方式）但是立方体是正方形的 3 维对应物，因此超立方体是立方体的 4 维对应物。

![超立方体](http://www.bottomupcs.com/chapter02/figures/hypercube.png)

上面我们可以看到外部多维数据集包含四个 8 个节点。 任何节点与另一个节点通信所需的最大路径数为 3.当另一个多维数据集放置在此多维数据集内时，我们现在有两倍的处理器数，但最大路径成本仅增加到 4.这意味着数量 处理器增长 2n，最大路径成本只能线性增长。

### 缓存一致性 (Cache Coherency)

仍然可以在 NUMA 系统中维护高速缓存一致性（这被称为高速缓存一致性 NUMA 系统或 ccNUMA）。正如我们所提到的，用于在 SMP 系统中保持处理器高速缓存一致性的基于广播的方案在大型 NUMA 系统中不能扩展到数百甚至数千个处理器。用于 NUMA 系统中的高速缓存一致性的一种常见方案被称为基于目录的模型。在此模型中，系统中的处理器与特殊缓存目录硬件进行通信。目录硬件为每个处理器保持一致的图像;这种抽象隐藏了 NUMA 系统从处理器的工作。

基于 Censier 和 Feautrier 目录的方案维护一个中心目录，其中每个存储器块具有标志位，称为每个处理器的有效位和单个脏位。当处理器将内存读入其缓存时，该目录会为该处理器设置有效位。

当处理器希望写入高速缓存行时，目录需要为存储器块设置脏位。这涉及向使用高速缓存行的那些处理器发送无效消息（并且只有那些设置了标志的处理器;避免广播流量）。

在此之后，任何其他处理器尝试读取内存块，目录将找到脏位设置。目录将需要从处理器获取当前设置的有效位的更新缓存行，将脏数据写回主存储器，然后将该数据提供回请求处理器，在此过程中为请求处理器设置有效位。请注意，这对请求处理器是透明的，并且目录可能需要从非常靠近或非常远的某个地方获取该数据。

显然，有数千个处理器与单个目录进行通信也不能很好地扩展。该方案的扩展涉及具有使用单独的协议在彼此之间通信的目录的层次结构。这些目录可以使用更通用的通信网络来相互通信，而不是 CPU 总线，从而允许扩展到更大的系统。

### NUMA Applications

NUMA 系统最适合需要处理器和内存之间进行大量交互的问题类型。例如，在天气模拟中，常见的习语是将环境划分为小的“盒子”，其以不同的方式响应（例如，海洋和陆地反射或存储不同的热量）。在进行模拟时，将输入较小的变化以查看总体结果。由于每个盒子影响周围的盒子（例如，更多的太阳意味着特定的盒子会产生更多的热量，影响它旁边的盒子）会有很多的通信（与渲染过程的各个图像帧形成对比，每个盒子都有不影响对方）。如果您对汽车碰撞进行建模，可能会发生类似的过程，其中模拟汽车的每个小盒子以某种方式折叠并吸收一定量的能量。

虽然软件并不直接知道底层系统是 NUMA 系统，但程序员在为系统编程时需要小心以获得最大性能。显然保持内存靠近将要使用它的处理器将产生最佳性能。程序员需要使用诸如分析之类的技术来分析所采用的代码路径以及他们的代码导致系统提取最佳性能的后果。

### 内存排序，锁定和原子操作

多级缓存，超标量多处理器体系结构带来了一些与程序员如何看待处理器运行代码有关的有趣问题。

想象一下，程序代码同时在两个处理器上运行，两个处理器有效地共享一个大面积的内存。如果一个处理器发出存储指令，将寄存器值放入存储器，何时可以确定另一个处理器是否负载该存储器，它将看到正确的值？

在最简单的情况下，系统可以保证如果程序执行存储指令，任何后续加载指令都将看到该值。这被称为严格的内存排序，因为规则不允许移动。您应该开始意识到为什么这类事情严重阻碍了系统的性能。

很多时候，内存排序不需要那么严格。程序员可以识别他们需要确保所有未完成的操作全局可见的点，但在这些点之间可能存在许多语义不重要的指令。

```c
typedef struct {
    int a;
    int b;
} a_struct;

    /*
     * Pass in a pointer to be allocated as a new structure
     */
void get_struct(a_struct *new_struct)
    {
    void *p = malloc(sizeof(a_struct));

    /* We don't particularly care what order the following two
    * instructions end up acutally executing in */
    p->a = 100;
    p->b = 150;
    /* However, they must be done before this instruction.
     * Otherwise, another processor who looks at the value of p
     * could find it pointing into a structure whose values have
     * not been filled out.
     */
    new_struct = p;
}
```

在这个例子中，我们有两个可以按任何特定顺序完成的存储，因为它适合处理器。但是，在最后一种情况下，只有在已知两个先前存储已完成时才必须更新指针。否则另一个处理器可能会查看 p 的值，跟随指向内存的指针，加载它，并得到一些完全不正确的值！

为了表明这一点，加载和存储必须具有描述它们必须具有的行为的语义。内存语义是根据围栏来描述的，这些围栏规定了如何在加载或存储周围重新排序加载和存储。

默认情况下，可以在任何地方重新订购装载或存储。

获取语 ​​ 义就像一个栅栏，只允许加载和存储向下移动。也就是说，当此加载或存储完成时，您可以保证任何后续加载或存储都将看到该值（因为它们不能在其上方移动）。

释放语义是相反的，它是一个围栏，允许任何一个或多个存储在它之前完成（向上移动），但在它向下移动之前没有任何东西。因此，当处理具有释放语义的加载或存储时，您可以存储任何先前的一个或多个存储已经完成。

![Acquire and Release semantics](http://www.bottomupcs.com/chapter02/figures/memorder.png)

完整的记忆围栏是两者的结合; 其中没有任何负载或存储可以在当前负载或存储周围的任何方向上重新排序。

最严格的内存模型将为每个操作使用完整的内存栅栏。 最弱的模型会将每个加载和存储作为正常的可重新订购指令。

### 处理器和内存模型

不同处理器实现不同的内存模型

x86（和 AMD64）处理器具有非常严格的内存模型; 所有 stores 都有发布(release)语义（也就是说，任何以后的加载或存储都可以保证 stores 的结果），但所有加载都具有正常的语义。 锁前缀给内存栅栏。

除非明确说明，否则 Itanium 允许所有加载和存储正常。XXX

### Locking

了解每个体系结构的内存排序要求对所有程序员来说都是不实际的，并且会使程序难以在不同的处理器类型上进行移植和调试。

程序员使用更高级别的抽象级别称为锁定，以便在有多个 CPU 时允许程序同时运行。

当程序获取对一段代码的锁定时，没有其他处理器可以获得锁定，直到它被释放。在任何关键代码之前，处理器必须尝试锁定;如果它不能拥有它，它就不会继续。

您可以在上一节中看到它如何与内存排序语义的命名联系起来。我们希望确保在获取锁之前，不会在其之前重新排序应该受锁保护的操作。这就是获取语义的工作原理。

相反，当我们释放锁时，我们必须确保在保持锁定时我们完成的每个操作都已完成（还记得先前更新指针的例子吗？）。这是发布语义。

有许多软件库可供程序员不必担心内存语义的细节，只需使用 lock（）和 unlock（）的更高级别的抽象。

#### Locking difficulties

锁定方案使编程更复杂，因为它可能使程序死锁。想象一下，如果一个处理器当前正在对某些数据进行锁定，并且当前正在等待锁定某些其他数据。如果其他处理器在解锁第二个锁之前等待锁定第一个处理器，我们就会遇到死锁情况。每个处理器都在等待另一个处理器，如果没有其他处理器，它们都无法继

这种情况通常是由于一种微妙的竞争条件而产生的;追踪中最困难的一个。如果两个处理器依赖于特定顺序发生的操作，则始终存在发生竞争条件的可能性。来自不同星系中爆炸恒星的伽马射线可能撞击其中一个处理器，使其跳过一个节拍，抛出操作的顺序。经常会发生如上所述的僵局。正是由于这个原因，程序排序需要通过语义来确保，而不是依赖于一次特定的行为。 （XXX 不确定我怎么能更好地说出来）。

类似的情况与死锁相反，称为活锁。避免死锁的一个策略可能是“礼貌”锁定;一个你放弃任何要求的人。这种礼貌可能会导致两个线程不断地给对方锁定，而无需锁定足够长的时间以完成关键工作并完成锁定（现实生活中的类似情况可能是两个人在一起遇到锁定门同时，都说“不，你先，我坚持”。最后都没有通过门！）

#### Locking strategies

在下面，有许多不同的策略来实现锁的行为。

一个简单的锁只有两种状态 - 锁定或解锁，被称为互斥（互斥的缩写;即如果一个人拥有它，另一个人不能拥有它）。

但是，有许多方法可以实现互斥锁。在最简单的情况下，我们有它通常所说的自旋锁。通过这种类型的锁定，处理器处于紧密的环路中等待锁定;相当于它说“我现在可以拥有它”，就像一个年幼的孩子可能会问父母一样。

这种策略的问题在于它基本上浪费了时间。虽然处理器不停地要求锁定，但它没有做任何有用的工作。对于可能仅在非常短的时间内保持锁定的锁，这可能是合适的，但在许多情况下，锁保持的时间可能相当长。

因此，另一种策略是sleep on a lock。在这种情况下，如果处理器无法进行锁定，它将开始执行其他工作，等待锁定可供使用的通知（我们将在后面的章节中看到操作系统如何切换进程并为处理器提供更多工作做）。

然而，互斥锁只是信号量的特例，荷兰计算机科学家Dijkstra发明了这种信号量。在存在多个可用资源的情况下，可以设置信号量以计算对资源的访问。在资源数量为1的情况下，您有一个互斥锁。信号量的操作可以在任何算法书中详细说明。

#### Atomic Operations

...